# data_loader.py
# AmaÃ§: YÃ¼ksek PerformanslÄ± ve Hata ToleranslÄ± Zemberek YÃ¼kleyici.
# Ã‡Ã¶zÃ¼m: Multiprocessing Pool ile analiz ve sonuÃ§larÄ± TSV dosyasÄ±na kaydetme.
# analysis_results.tsv dosyasÄ±nÄ± oluÅŸturur.
# Gemini (Verimli SQLite Veri YÃ¼kleme Stratejisi)

import os
import glob
import sqlite3
import sys
import csv
from typing import List, Tuple
from multiprocessing import Pool, cpu_count, current_process

# Zemberek/Jpype kÃ¼tÃ¼phaneleri
try:
    from jpype import startJVM, getDefaultJVMPath, JClass, JString, shutdownJVM
except ImportError:
    print("HATA: jpype1 kÃ¼tÃ¼phanesi kurulu deÄŸil. LÃ¼tfen kontrol edin.")
    sys.exit(1)

# --- GLOBAL KONFÄ°GÃœRASYONLAR ---
morphology = None
TurkishMorphology = None
ZEMBEREK_PATH = os.path.abspath('zemberek-full.jar')
TSV_OUTPUT_FILE = 'analysis_results.tsv'

# --- ZEMBEREK Ä°ÅÃ‡Ä° FONKSÄ°YONLARI ---

def zemberek_pool_worker_init():
    """
    Bu fonksiyon, her iÅŸÃ§i sÃ¼reci (worker) baÅŸladÄ±ÄŸÄ±nda YALNIZCA BÄ°R KEZ Ã§alÄ±ÅŸÄ±r.
    JVM'yi baÅŸlatÄ±r ve Zemberek'i belleÄŸe yÃ¼kler (Tek seferlik maliyet).
    """
    global morphology, TurkishMorphology
    
    try:
        # Her iÅŸÃ§iye daha az RAM veriyoruz (32GB RAM iÃ§in 2GB yeterli)
        startJVM(getDefaultJVMPath(), '-ea', f'-Djava.class.path={ZEMBEREK_PATH}', '-Xmx2g') 
        TurkishMorphology = JClass('zemberek.morphology.TurkishMorphology')
        morphology = TurkishMorphology.createWithDefaults()
        # print(f"-> Ä°ÅŸÃ§i {current_process().pid}: Zemberek HazÄ±r.") # GÃ¼rÃ¼ltÃ¼yÃ¼ azaltmak iÃ§in kapatÄ±ldÄ±
    except Exception as e:
        print(f"-> Ä°ÅŸÃ§i {current_process().pid}: JVM/Zemberek BaÅŸlatma HatasÄ±: {e}", file=sys.stderr)
        sys.exit(1)

def format_morphemes(analysis):
    """Ekleri (iyor-um) formatÄ±nda Ã§Ä±karÄ±r."""
    morpheme_data_list = analysis.getMorphemeDataList()
    if morpheme_data_list.size() <= 1: 
        return ""
    surface_forms = [str(morpheme_data_list.get(i).surface) for i in range(1, morpheme_data_list.size())]
    return f"({'-'.join(surface_forms)})"

def analyze_single_word(word: str) -> Tuple:
    """
    Her kelimeyi analiz eder (Ä°ÅŸÃ§i Pool'u tarafÄ±ndan tekrar tekrar kullanÄ±lÄ±r).
    """
    global morphology
    
    if morphology is None:
        return (word, "", "", "", "HATA: JVM baÅŸlatÄ±lamadÄ±", "zemberek_hata")

    try:
        j_word = JString(word)
        analysis = morphology.analyze(j_word) 
        results = analysis.getAnalysisResults()
        
        if results.isEmpty():
            return (word, word, "", "", "", "zemberek")
        
        best_result = results.get(0)
        
        lemma = str(best_result.getLemmas()[0])
        kok = str(best_result.getStems()[0])
        ekler = format_morphemes(best_result)
        analiz_tam = str(best_result.formatLong())
        yontem = "zemberek"
        
        # TSV'ye yazÄ±lacak format: kelime, lemma, kok, ekler, analiz, yontem
        return (word, lemma, kok, ekler, analiz_tam, yontem)

    except Exception as e:
        # Analiz sÄ±rasÄ±nda takÄ±lma/Ã§Ã¶kme yaÅŸanÄ±rsa, hata yakalanÄ±r ve atlanÄ±r.
        return (word, word, "", "", f"HATA: Analiz HatasÄ± ({e})", "zemberek_hata")

# --- VERÄ°TABANI YÃ–NETÄ°MÄ° ---

class DBManager:
    def __init__(self, db_path: str = 'lexicon.db'):
        self.db_path = db_path
        self.tsv_path = TSV_OUTPUT_FILE

    def setup_database(self):
        """SQLite veritabanÄ± tablolarÄ±nÄ± oluÅŸturur/gÃ¼nceller."""
        print(f"-> VeritabanÄ± {self.db_path} ve tablolar oluÅŸturuluyor/gÃ¼ncelleniyor...")
        script = """
            CREATE TABLE IF NOT EXISTS kelimeler (
                id INTEGER PRIMARY KEY,
                kelime TEXT NOT NULL UNIQUE,
                lemma TEXT,
                kok TEXT,
                ekler TEXT,
                analiz TEXT,
                yontem TEXT,
                aciklama TEXT,
                onay INTEGER DEFAULT 0,
                CHECK (LENGTH(kelime) > 0)
            );
        """
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.cursor().executescript(script)
            print("-> VeritabanÄ± yapÄ±sÄ± baÅŸarÄ±yla oluÅŸturuldu.")
        except sqlite3.Error as e:
            print(f"VeritabanÄ± kurulum hatasÄ±: {e}")

    def import_tsv_to_db(self):
        """TSV dosyasÄ±ndaki verileri toplu olarak veritabanÄ±na yÃ¼kler.
        Ancak bu metodu bu betikte kullanmayacaÄŸÄ±z."""
        if not os.path.exists(self.tsv_path):
            print(f"UYARI: TSV dosyasÄ± ({self.tsv_path}) bulunamadÄ±. VeritabanÄ±na aktarÄ±lacak bir ÅŸey yok.")
            return 0
        
        print(f"\n-> '{self.tsv_path}' dosyasÄ±ndan veritabanÄ±na toplu yÃ¼kleme baÅŸlatÄ±lÄ±yor...")
        total_imported = 0
        
        # Kelime, lemma, kÃ¶k, ekler, analiz, yÃ¶ntem
        sql = "INSERT OR IGNORE INTO kelimeler (kelime, lemma, kok, ekler, analiz, yontem) VALUES (?, ?, ?, ?, ?, ?)"
        
        try:
            with open(self.tsv_path, 'r', encoding='utf-8') as f:
                reader = csv.reader(f, delimiter='\t')
                data = list(reader) # TÃ¼m veriyi belleÄŸe al

            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.executemany(sql, data)
                conn.commit()
                total_imported = cursor.rowcount # rows imported
            
            print(f"-> VeritabanÄ±na {total_imported} satÄ±r baÅŸarÄ±yla eklendi/gÃ¼ncellendi.")
            return total_imported

        except sqlite3.Error as e:
            print(f"VeritabanÄ± aktarÄ±m hatasÄ±: {e}")
            raise
        except Exception as e:
            print(f"TSV okuma/aktarma hatasÄ±: {e}")
            raise


# --- ANA Ä°Å AKIÅI ---

def process_chunk_and_load(chunk_file: str, num_processes: int, batch_size: int = 5000):
    
    # 1. Kelimeleri Oku
    with open(chunk_file, 'r', encoding='utf-8') as f:
        words = [line.strip() for line in f if line.strip()]
        
    total_words = len(words)
    print(f"\n--- ğŸ“‚ {chunk_file} iÅŸleniyor. Toplam {total_words} kelime bulundu. ---")
    
    print(f"-> {num_processes} adet paralel Zemberek iÅŸÃ§isi (worker) baÅŸlatÄ±lÄ±yor. (Bu birkaÃ§ saniye sÃ¼recek)...")
    
    total_processed = 0
    
    # TSV dosyasÄ±na veriyi yazmak iÃ§in aÃ§
    # 'a': append modu. EÄŸer program Ã§Ã¶kerse, yeniden baÅŸlarken kaldÄ±ÄŸÄ± yerden devam edebilir.
    with open(TSV_OUTPUT_FILE, 'a', encoding='utf-8', newline='') as tsvfile:
        tsv_writer = csv.writer(tsvfile, delimiter='\t', quoting=csv.QUOTE_MINIMAL)

        # 2. Pool'u BaÅŸlat (JVM'ler bu aÅŸamada yÃ¼klenir)
        with Pool(processes=num_processes, initializer=zemberek_pool_worker_init) as pool:
            
            # 3. Kelimeleri batch'ler halinde pool'a gÃ¶nder ve sonuÃ§larÄ± TSV'ye yaz
            
            for i in range(0, total_words, batch_size):
                word_batch = words[i:i + batch_size]
                batch_num = i // batch_size + 1
                
                # Paralel Analiz: pool.map ile tÃ¼m batch'i gÃ¶nder
                # EÄŸer bir worker kilitlenirse, pool.map tÃ¼m sonuÃ§larÄ± bekler.
                # Bu yÃ¼zden Zemberek kilitlenmelerini analyze_single_word iÃ§inde yakaladÄ±k.
                analyzed_data = pool.map(analyze_single_word, word_batch)
                
                # 4. TSV DosyasÄ±na Yazma (Ana sÃ¼reÃ§)
                tsv_writer.writerows(analyzed_data)
                
                total_processed += len(analyzed_data)
                
                # Ä°lerleme Takibi
                print(f"[{chunk_file}] Ä°lerleme: {total_processed} kelime iÅŸlendi. Batch {batch_num} TSV'ye yazÄ±ldÄ±.") 
                
    print(f"--- âœ… {chunk_file} iÅŸlenmesi tamamlandÄ±. Toplam {total_processed} kelime analiz edildi. ---")

def main():
    print("--- TÃ¼rkÃ§e Leksikon VeritabanÄ± YÃ¼kleyici (Optimal TSV YÃ¶ntemi) ---")
    
    # KullanÄ±labilir CPU Ã§ekirdek sayÄ±nÄ±zÄ±n bir kÄ±smÄ± kullanÄ±lÄ±r (32GB RAM iÃ§in 6 uygun bir baÅŸlangÄ±Ã§tÄ±r)
    num_processes = min(cpu_count(), 6) 
    print(f"Sistem CPU SayÄ±sÄ±: {cpu_count()}. KullanÄ±lan Ä°ÅŸÃ§i SayÄ±sÄ±: {num_processes}.")
    
    db_manager = DBManager(db_path='lexicon.db')
    db_manager.setup_database() 

    # 1. Analiz ve TSV DosyasÄ±na Yazma
    chunk_files = sorted(glob.glob('chunk_*.txt'))
    
    if not chunk_files:
        print("HATA: 'chunk_?.txt' formatÄ±nda dosya bulunamadÄ±.")
        return

    for chunk_file in chunk_files:
        process_chunk_and_load(chunk_file, num_processes, batch_size=5000)

    # 2. Analiz tamamlandÄ±ktan sonra TSV'den veritabanÄ±na yÃ¼kleme iÅŸlemini db_loader.py ile yapacaÄŸÄ±z.
    # total_imported = db_manager.import_tsv_to_db()

    print(f"\n\nğŸ‰ TÃœM Ä°ÅLEMLER BAÅARIYLA TAMAMLANDI!")
    # print(f"   Analiz edilen toplam kelime: {total_imported}")


if __name__ == "__main__":
    main()
